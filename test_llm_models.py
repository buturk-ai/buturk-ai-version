from core.ai_engine.llm_router import run_model_direct

models = ["llama3:latest", "codellama:34b", "deepseek-coder:latest"]
prompt = "Ø§ÙƒØªØ¨ ÙƒÙˆØ¯ Ø¨Ø§ÙŠØ«ÙˆÙ† ÙŠØ·Ø¨Ø¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ù† 1 Ø¥Ù„Ù‰ 10"

for model in models:
    print(f"\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model}")
    try:
        result = run_model_direct(model=model, prompt=prompt)

        output = ""
        if result.stdout:
            output = result.stdout if isinstance(result.stdout, str) else result.stdout.decode('utf-8', errors='ignore')
        elif result.stderr:
            output = result.stderr if isinstance(result.stderr, str) else result.stderr.decode('utf-8', errors='ignore')

        print(f"âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©:\n{output.strip() if output else 'ğŸš« Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù†ØªÙŠØ¬Ø©'}\n")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model}: {e}")
